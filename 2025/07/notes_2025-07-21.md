[Home](../../main.md) | [Prev: Day 191](notes_2025-07-18.md) | [Next: Day 193](./notes_2025-07-22.md)

## 📝 Day 193, Monday - `notes_2025-07-21.md`


### Meeting notes
- during meeting mentioned CCEA and ALCOA data integrity standards
- CCEA, in the context of data integrity, generally refers to the ALCOA+ principles (Attributable, Legible, Contemporaneous, Original, Accurate, Complete, Consistent, Enduring, and Available), which are a set of guidelines for ensuring the reliability and trustworthiness of data, particularly in regulated industries like pharmaceuticals. The "CCEA" portion of ALCOA+ specifically stands for Complete, Consistent, Enduring, and Available. These principles ensure data is reliable, verifiable, and can be trusted throughout its lifecycle.

Elaboration: 

• ALCOA+ Principles: ALCOA+ builds upon the original ALCOA principles (Attributable, Legible, Contemporaneous, Original, and Accurate) by adding "Complete," "Consistent," "Enduring," and "Available".
• Attributable: Data should be traceable back to its source (who collected it and when).
• Legible: Data should be easily readable and understandable.  
• Contemporaneous: Data should be recorded at the time it was generated.  
• Original: Data should be the original record or a verified copy.  
• Accurate: Data should be free from errors and represent the true state of affairs.  
• Complete: All data related to a process or experiment should be recorded.  
• Consistent: Data should be consistent across different records and systems.  
• Enduring: Data should be stored in a way that ensures its long-term integrity and accessibility.  
• Available: Data should be readily accessible to authorized users when needed.  

Importance in Regulated Industries: 

• Pharmaceuticals: These principles are crucial in the pharmaceutical industry for maintaining the quality, safety, and efficacy of drugs.  
• Clinical Trials: Data integrity is vital for ensuring the reliability of clinical trial results.  
• Regulatory Compliance: Adhering to ALCOA+ principles helps organizations meet regulatory requirements (e.g., from the U.S. Food and Drug Administration (.gov) or other regulatory bodies).  

In essence, ALCOA+ provides a framework for ensuring that data is reliable, traceable, and can be trusted throughout its lifecycle.  

- sent powerpoint slide (`Mid-Year_R&D-ix_Update_2_LT_30-JUNE-2025.ppt`) of informatics work done up to date


### US340
- visualize git commit stats to display work done up to date
    * worked on `git_commit_stats_visualise.ipynb` file to generate plot images and traceable

| WorkID | TITLE |
|------ | -----|
| US261 | Create new modular SUMM_JAK2 and test for equivalency |
| US224 | Run a query of CRO Experiments and capture a list of IDs that have question mark patterns |
| US292 | investigate api error logs on PROD |
| US167 | Create python script to execute API calls on upgraded version 6 DM server & original DM server |
| BG131 | JAK JH2 TYK2 IC50 modifier is reversed |
| US261 | Create new modular SUMM_JAK2 and test for equivalency |
| US153 | KAT6 Browser Summary page modifications from Min W. 1-NOV-2024 |
| US201 | Create visuals on python API script data & export exp ids |
| US191 | Add validation checkboxes to ELN_Writeup Notebook  |
| US286 | barrage of DTX emails (errors) |
| US216 | web scrape the ~3300 exp ids for accurate comparison |
| US162 | Modify the KAT6 summary page for Min -  |
| US320 | Duplicate/Missing Data for MCF7 Assays in Dotmatics |
| US280 | Spencer to investigate audit trails for batch 002 or batch 003 compounds |
| US193 | Add new data table from SMARCA2 to the KAT6A program summary page.  |
| US194 | Review python script and API call from Dotmatics  |
| US160 | Add proliferation data to JAK2 summary page  |
| US176 | Add project 274 to the SMARCA project page.  |


### US337
- setup mysql db on IX01 server
    * need to use older version `8.0` of mysql for compatibility
    * removed container and image and re-ran different command below
    * ensure the version is 8.0 and not 8.0.0
    * Corrupt/Incompatible MySQL data files in the volume; `docker volume rm mysql_data`
    * The volume mysql_data might have been initialized by another version or got corrupted.
    * Use of unstable MySQL version
    * MySQL 8.0.0 is a very early DMR version and has known stability issues. Consider using mysql:8.0 or mysql:8.0.36 (latest stable) instead.

```bash

docker run -d  --name mysql-server  -e MYSQL_ROOT_PASSWORD=${ROOT_PASS}  -e MYSQL_DATABASE=preludetx  -e MYSQL_USER=preludetx  -e MYSQL_PASSWORD=${PASS}  -v mysql_data:/var/lib/mysql  -p 3306:3306  --restart unless-stopped  mysql:8.0
```

- ran random sql to check if database is working

```sql
SELECT
    UUID() AS random_id,
    CONCAT('User_', FLOOR(RAND() * 10000)) AS fake_username,
    CONCAT(FLOOR(RAND() * 100), '@example.com') AS fake_email,
    FLOOR(18 + (RAND() * 50)) AS fake_age,
    NOW() - INTERVAL FLOOR(RAND() * 365) DAY AS signup_date,
    IF(RAND() > 0.5, 'active', 'inactive') AS status,
    ROUND(PI() * RAND(), 5) AS pi_variation,
    ELT(FLOOR(1 + RAND() * 5), 'red', 'green', 'blue', 'yellow', 'purple') AS favorite_color,
    SHA2(CONCAT(UUID(), RAND()), 256) AS hashed_token,
    (SELECT COUNT(*) FROM information_schema.tables) AS table_count,
    LEFT(REPEAT(CONCAT(FLOOR(RAND() * 10)), 50), 100) AS synthetic_pattern,
    NOW() + INTERVAL FLOOR(RAND() * 10000) SECOND AS future_event_time
FROM DUAL;
```

### US339
- add delayed-testing to any work items that have higher than 20 day turn-around time
    * WIQL cannot do date arithmetic or duration filtering — that must be done client-side.
    * Azure DevOps WIQL response returns work item IDs; details require separate batch API calls.
    * Use jq's date functions with fractional seconds stripped for date parsing.
    * Use zsh arrays to split large ID lists into manageable chunks for API requests.
    * Combine curl and jq to automate filtering and extraction of required data.


#### Summary
- Filter work items where the completion time (ClosedDate - CreatedDate) is greater than 20 days
- Extract only the work item IDs for those filtered items

#### 1: WIQL Query to get Closed Work Items with relevant dates

```bash
read -r -d '' WIQL_QUERY << EOM
{
  "query": "
    SELECT
      [System.Id],
      [System.CreatedDate],
      [Microsoft.VSTS.Common.ClosedDate]
    FROM WorkItems
    WHERE
      [System.TeamProject] = 'PreludeTx_Dotmatics_2024' AND
      [System.State] = 'Closed'
    ORDER BY [System.ChangedDate] DESC
  "
}
EOM
```

#### 2: Using curl to execute WIQL and store response

```bash
WIQL_RESPONSE=$(curl -s -u ":$AZURE_DEVOPS_PAT" \
  -H "Content-Type: application/json" \
  -X POST \
  -d "$WIQL_QUERY" \
  "$API_URL")
```

#### 3: Extract Work Item IDs from WIQL response

```bash
IDS=$(echo "$WIQL_RESPONSE" | jq -r '.workItems[].id' | paste -sd,)
```

#### 4: Chunk `IDS` to satisfy 200 item limit on API call 

```bash
# Convert comma-separated IDS into array
IDS_ARRAY=(${(s:,:)IDS})

# Split into two chunks (first 115, second rest)
CHUNK1=(${IDS_ARRAY[1,115]})
CHUNK2=(${IDS_ARRAY[116,-1]})
```


#### 5: Fetch detailed work item data in chunks

```bash
WORK_ITEM_1=$(curl -s -u ":$AZURE_DEVOPS_PAT" \
  -H "Content-Type: application/json" \
  "$ORG/_apis/wit/workitems?ids=$(IFS=,; echo "${CHUNK1[*]}")&fields=System.Id,System.CreatedDate,Microsoft.VSTS.Common.ClosedDate")

WORK_ITEM_2=$(curl -s -u ":$AZURE_DEVOPS_PAT" \
  -H "Content-Type: application/json" \
  "$ORG/_apis/wit/workitems?ids=$(IFS=,; echo "${CHUNK2[*]}")&fields=System.Id,System.CreatedDate,Microsoft.VSTS.Common.ClosedDate")
```

#### 6: Filter work items where completion time > 20 days using `jq`

- Use of `sub("\\.[0-9]+Z$"; "Z")`
    * Azure DevOps returns timestamps with fractional seconds, e.g. 2024-07-16T11:05:13.68Z which fromdateiso8601 cannot parse.
    * This strips the fractional seconds to make the date parseable by `jq`
    * The difference is calculated in seconds: `60 * 60 * 24 * 20 =` number of seconds in 20 days
    * Output is filtered work item IDs with completion time > 20 days

```bash
echo "$WORK_ITEM_1" | jq -r '
  .value[] |
  select(
    ((.fields["Microsoft.VSTS.Common.ClosedDate"] | sub("\\.[0-9]+Z$"; "Z")) | fromdateiso8601) -
    ((.fields["System.CreatedDate"] | sub("\\.[0-9]+Z$"; "Z")) | fromdateiso8601) > (60*60*24*20)
  ) | .id
'
```

#### Result
- this comma-delimited list was used in the wisql to query only for `delayed` or `on-hold` work items; then used in the dashboard bar chart

```
106,112,115,119,128,130,131,136,137,139,141,143,144,153,154,156,160,162,164,167,171,174,175,176,177,178,183,191,196,208,209,211,222,233,234,240,242,255,261,266,273,275,277,282
```
