[Home](../../main.md) | [Prev: Day 227](notes_2025-09-08.md) | [Next: Day 229](./notes_2025-09-10.md)

## üìù Day 228, Tuesday - `notes_2025-09-09.md`

### US363
- Modify BioRegister to accommodate dual-payload linkers
    * investigate sql query to get payload to target mapping
    * the final version sql has a caveat, the order of the payloads and/or targets need to be consistent and cannot have one-off custom mappings since they are mapped based on order they were inputted
    * Payloads and targets are mapped by position: 1st-to-1st, 2nd-to-2nd, 3rd-to-3rd. This order is fixed at input time and cannot be changed afterwards
    * Payloads and targets are paired strictly by order of entry
       * The 1st payload is mapped to the 1st target
       * The 2nd payload is mapped to the 2nd target
       * The 3rd payload is mapped to the 3rd target
    * If the order is desired to be changed, the admin needs to manually change the id of the payloads/targets using an `UPDATE` statement
    * If duplicate targets or payloads are required, cannot use this logic; would need to create n-number of drop-down boxes for target and payloads; but the n-number of dropdown menus cannot be dynamically set; only a custom webapp can do this; would need to use a postgres db to store this info and then have a external connection from DTX using JDBC in the `Connections` tab

```sql
-- showing the id's of payload and target - TESTING
SELECT distinct
    b.bioreg_id,
    g.formatted_id   AS payload_linker,
    f.value          AS payload,
    f.id             AS payload_id,
    d.value          AS payload_target,
    d.id             AS string_value_id,
    ROW_NUMBER() OVER (PARTITION BY a.child_id ORDER BY f.id, d.id) AS payload_target_pair
FROM (
        SELECT
            b.id,
            b.bioreg_id,
            a.child_id
        FROM
            c2c.complex_child a
            JOIN c2c.complex b ON b.id = a.complex_id
     ) a
JOIN (
        SELECT
            *
        FROM
            c2c.complex_child a
            JOIN c2c.complex b ON b.id = a.child_id
        WHERE
            upper(composition) NOT LIKE '%DELETED%'
     ) b
  ON b.child_id = a.child_id
LEFT JOIN (
        SELECT
            b.id,
            b.value,
            b.owner_id
        FROM
            c2c.string_value_type a
            JOIN c2c.string_value b ON a.id = b.type_id
        WHERE b.type_id = 15
     ) f
  ON a.child_id = f.owner_id
LEFT JOIN (
        SELECT
            b.id,
            b.value,
            b.owner_id
        FROM
            c2c.string_value_type a
            JOIN c2c.string_value b ON a.id = b.type_id
        WHERE b.type_id = 16
     ) d
  ON a.child_id = d.owner_id
LEFT JOIN (
        SELECT
            a.bioreg_id,
            b.formatted_id
        FROM
            ds3_userdata.adc_conjugate_vw a
            JOIN c$pinpoint.reg_data b ON a.formatted_id = b.formatted_id
     ) g
  ON g.bioreg_id = b.bioreg_id
  where b.bioreg_id = 'PRT5000447'


-- final version that de-duplicates and shows correct ranking mapping:
WITH base AS (
    SELECT DISTINCT
        b.bioreg_id,
        g.formatted_id   AS payload_linker,
        f.value          AS payload,
        f.id             AS payload_id,
        d.value          AS payload_target,
        d.id             AS string_value_id
    FROM (
            SELECT
                b.id,
                b.bioreg_id,
                a.child_id
            FROM
                c2c.complex_child a
                JOIN c2c.complex b ON b.id = a.complex_id
         ) a
    JOIN (
            SELECT *
            FROM c2c.complex_child a
            JOIN c2c.complex b ON b.id = a.child_id
            WHERE upper(composition) NOT LIKE '%DELETED%'
         ) b
      ON b.child_id = a.child_id
    LEFT JOIN (
            SELECT b.id, b.value, b.owner_id
            FROM c2c.string_value_type a
            JOIN c2c.string_value b ON a.id = b.type_id
            WHERE b.type_id = 15
         ) f
      ON a.child_id = f.owner_id
    LEFT JOIN (
            SELECT b.id, b.value, b.owner_id
            FROM c2c.string_value_type a
            JOIN c2c.string_value b ON a.id = b.type_id
            WHERE b.type_id = 16
         ) d
      ON a.child_id = d.owner_id
    LEFT JOIN (
            SELECT a.bioreg_id, b.formatted_id
            FROM ds3_userdata.adc_conjugate_vw a
            JOIN c$pinpoint.reg_data b ON a.formatted_id = b.formatted_id
         ) g
      ON g.bioreg_id = b.bioreg_id
    WHERE b.bioreg_id = 'PRT5000447'
),
payloads AS (
    SELECT payload_id, payload, MAX(bioreg_id) AS bioreg_id, MAX(payload_linker) AS payload_linker,
           ROW_NUMBER() OVER (ORDER BY payload_id) rn
    FROM base
    GROUP BY payload_id, payload
),
targets AS (
    SELECT string_value_id, payload_target,
           ROW_NUMBER() OVER (ORDER BY string_value_id) rn
    FROM base
    GROUP BY string_value_id, payload_target
)
SELECT
    p.bioreg_id,
    p.payload_linker,
    p.payload,
    p.payload_id,
    t.payload_target,
    t.string_value_id,
    p.rn AS payload_target_pair
FROM payloads p
JOIN targets t ON p.rn = t.rn
ORDER BY p.rn
```

### US277
- Assist TS tech support in diagnosing inject dotmatics into mosaic pipeline
    * weekly meeting on Tues; kick-off today
    * `https://github.com/tetrascience/ts-client-preludetx-task-script-dotmatics-to-mosaic` is the repo for hosting the python/js data for the pipelines
    * will allow as external collaborator so will need to go thru change request approval procedure (1-2 days)
        * some approval teams work in UK (GMT)


### US364
- Ensure PRT5000495-002 is deleted from the backend of BioRegister
    * DTX came back with:
    * >As a standard practice, we avoid performing direct deletes in the database to prevent any unintended downstream impacts. This approach helps ensure data integrity and system stability across all integrated components.
    * seems next step is to do nothing and just accept the batch data in backend, however ignore it; since it most likely will not impact anything
    * from docs:

    * >### Soft delete related options
    * >The app { } block has a number of options related to soft deletes.¬†**Bioregister**¬†can use soft delete, whch will merely mark an entity as deleted, not delete the actual row of data. A¬†**soft delete**¬†will automatically soft delete entities that depend on it (such as batches). These will be listed before the delete is carried out. Re-instating a deleted entity will not automatically re-instate such dependent entities (i.e. batches of deleted entities will have to be re-instated separately). Deleted entities will be taken out of consideration for duplicate checking, so it will be possible to register an exact duplicate of a soft-deleted entity. A subsequent attempt to re-instate such a deleted entity would fail due to a uniqueness clash with this new entity. Deleted entities can be view via the icon in the bottom lists. Deleted entities can be re-instated from the 'view' page for an individual entity. If a soft-deleted items is deleted, it will permanently be deleted (removed from tables).
    * in summary, it soft deletes the batch data, and thus in the backend it still exists, this setting is intended for `Enable soft deletes (check to enable. Requires system restart)`; thus this is considered closed


### US348
- Create a tool to transform user Mass-Spec data to a standardized analysis format
    * create `clear_outputs_on_open.py` to clear cell output on save
    * add to `Documents/jupyter_extensions`
    * the idea did not work: the extension modifies the notebook model in memory when it‚Äôs loaded, but JupyterLab immediately sends the kernel‚Äôs outputs back to the notebook once the kernel connects. That means: the extension does run (no errors in logs)
        * But the outputs seen in the notebook are being repopulated by the live kernel after the extension clears them
        * So clearing outputs on ‚Äúopen‚Äù alone cannot override a running kernel that already has outputs
    * need to tell client to clear output each time


```bash
docker run -d -p 8888:8888 `
  -v "C:\Users\Disco-lx\Documents:/home/jovyan/work" `
  -v "C:\Users\Disco-lx\Documents\jupyter_extensions:/home/jovyan/jupyter_extensions" `
  jupyter/base-notebook bash -c @"
pip install pandas xlsxwriter ipywidgets
mkdir -p /home/jovyan/.jupyter
cat <<EOT > /home/jovyan/.jupyter/jupyter_notebook_config.py
import sys
sys.path.append(\"/home/jovyan/jupyter_extensions\")
c.ServerApp.jpserver_extensions = {\"clear_outputs_on_open\": True}
EOT
start-notebook.sh --NotebookApp.notebook_dir=/home/jovyan/work/notebooks --NotebookApp.token='' --NotebookApp.ip=0.0.0.0
"@
```

- the extension code to clear content

```python
from notebook.utils import to_api_path
from notebook.services.contents.manager import ContentsManager

def _clear_outputs_on_open(contents_manager: ContentsManager, model, path):
    if model['type'] == 'notebook':
        for cell in model.get('content', {}).get('cells', []):
            if 'outputs' in cell:
                cell['outputs'] = []
            if 'execution_count' in cell:
                cell['execution_count'] = None

def load_jupyter_server_extension(nbapp):
    original_get = nbapp.contents_manager.get

    def new_get(path, *args, **kwargs):
        model = original_get(path, *args, **kwargs)
        _clear_outputs_on_open(nbapp.contents_manager, model, path)
        return model

    nbapp.contents_manager.get = new_get
    nbapp.log.info("[clear_outputs_on_open] Extension loaded")
```

- there is a manual way to clear output:

```bash
jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace /home/jovyan/work/notebooks/acetylation_analysis_client.ipynb
```
