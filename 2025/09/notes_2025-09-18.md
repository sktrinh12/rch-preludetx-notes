[Home](../../main.md) | [Prev: Day 234](notes_2025-09-17.md) | [Next: Day 236](./notes_2025-09-19.md)

## üìù Day 235, Thursday - `notes_2025-09-18.md`

### US367
- create diagram of TS pipelines for pherastar 
    * client requested a diagram for visualising the connections of the pipelines

```
flowchart TD

    %% Annotation styling
    classDef noteStyle fill:#fff2cc,stroke:#d6b656,stroke-width:2px,color:#000,stroke-dasharray: 5 5

    %% Subgraph to group D with its note
    subgraph S1 [" "]
        direction LR
        D["Post Tecan IDS to Mosaic Samplebank<br>Trigger: RAW file type<br>Path: ./screening/pherastar_uat/*.txt"]
        N[["üìù Note:<br>Backdoor method ‚Äì convert Tecan raw data ‚Üí PheraStar raw data,<br>then trigger Decorate pipeline",<br>user must export pherastar data as .csv]]:::noteStyle
    end

    %% Other pipeline nodes
    A["Decorate RAW PheraStar data<br>Trigger: RAW file type<br>Path: ./screening/pherastar_uat/*.csv"]
    B["BMG Labtech Pherastar RAW to IDS<br>(Decorate RAW PheraStar data pipeline)"]
    C["BMG Labtech Pherastar IDS to Dotmatics<br>(Depends on BMG Labtech Pherastar RAW to IDS)"]

    %% Flow connections
    D -->|Uploads Tecan data to Mosaic<br>Required before Decorate pipeline| A
    A --> B
    B -->|Requires data to already be in Mosaic| C

    %% Hide subgraph border
    style S1 fill:transparent,stroke:transparent
```


### Research on ChromaDB vector database and agent/llm model
- go thru `Getting Started` Docs from [Chromadb](https://docs.trychroma.com/docs/overview/getting-started)
    * the basic premise is to: upload `json` data to a vector db, chromadb, the data is from azure devops kanban board. after scarping all the data and comments, description, etc and saved as a json, cleaned up the json to prepare for upload:
    * implement a chat agent that would be based off of a base llm model and feed it information scraped from devops board from all user stories and bugs and tasks so that it is an expert at everything done up to a certain point in time where someone would run a pipeline to update its knowledge-base from new user stories and work items, the agent/model wouldn't provide feedback on things in the future or something that hasn't happened yet.  It would only use what information it was given from azure devops work items. The purpose of the agent would be to provide quick answers to a user, rather than the user searching for keywords of work items
    * created three `.py` files to scrape azure devops kanban board data; clean and prepare data; and finally upload to `chromadb` vector database
        * `scrape_az_workitems.py`
        * `clean_workitems.py`
        * `upload_chromadb.py`
